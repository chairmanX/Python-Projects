{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data: getting full text from all the .docx interview files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get all text from a file \n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = \"\"\n",
    "    for para in doc.paragraphs: \n",
    "        fullText = fullText + para.text + \" \"\n",
    "    return fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\605665\\\\Documents\\\\Student2Student\\\\Interview Notes'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change working directory\n",
    "interview_folder = \"C:\\\\Users\\\\605665\\\\Documents\\\\Student2Student\\\\Interview Notes\"\n",
    "os.chdir(interview_folder)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = [] #list for all the interview text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to loop through all the interview notes documents \n",
    "for filename in os.listdir(interview_folder):\n",
    "    allText.append(getText(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing:\n",
    "- Tokenization: split text, lowercase, remove puncutation \n",
    "- removing words with fewer than 3 characters\n",
    "- removing stopwords \n",
    "- lemmatized: grouping different inflections of words together \n",
    "- stemmed: words reduced to root form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\605665\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "#lemmatize example\n",
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new instance of an english stemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization and stemming\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['As', 'a', 'family,', 'we', 'moved', 'from', 'Omaha', 'to', 'Ft.', 'Meade', '(2004),', 'Ft.', 'Meade', 'to', 'Ft.', 'Bragg', '(2007),', 'and', 'Ft.', 'Bragg', 'to', 'Joint', 'Base', 'San', 'Antonio', 'Randolph', '(2012)', '', '', 'Our', 'first', 'family', 'move,', 'my', 'kids', 'were', '2', 'yrs', 'old', 'and', '2', 'months', 'old;', 'respectively.', '', 'My', 'wife', 'and', 'I', 'were', '28.', '', '', '', 'For', 'the', 'first', 'move,', 'we', 'really', 'relied', 'on', 'sponsor', 'packages', 'and', 'any', 'material', 'we', 'could', 'find;', 'internet,', 'family', 'support', 'center,', 'etc.', '', 'We', 'purposely', 'stayed', 'on', 'base', 'because', 'we', 'had', 'no', 'idea', 'what', 'to', 'expect.', '', 'For', 'subsequent', 'moves,', 'the', 'internet', 'was', 'helpful,', 'but', 'we', 'also', 'relied', 'very', 'heavily', 'on', 'our', 'friend', 'networks', 'to', 'get', 'ground', 'truth', 'on', 'school', 'districts,', 'doctors,', 'neighborhoods,', 'commute', 'times,', 'etc.', '', 'We', 'never', 'relied', 'on', 'anything', 'branch', 'specific.', '', '', '', '', 'It', 'seems', 'as', 'though', 'it', 'was', 'very', 'difficult', 'for', 'Military', 'Spouses', 'to', 'make', 'close', 'relationships.', '', 'Very', 'caste', 'societal', \"(what's\", 'your', 'spouse', 'do?', '', 'lots', 'of', 'bona', 'fides', 'exchanged),', 'and', 'the', 'fear', 'of', 'moving', 'was', 'generally', 'guarded', 'against', 'becoming', 'to', 'close', 'to', 'anyone', 'outside', 'of', 'immediate', 'peers', 'or', 'neighbors.', '', 'Kids', 'seemed', 'pretty', 'adaptive;', 'it', 'really', 'becomes', 'their', 'normal.', '', '', '', 'We', 'used', 'the', 'housing', 'office', 'to', 'save', 'on', 'utility', 'start', 'up', 'costs.', '', 'And,', 'usually', 'the', 'child', 'development', 'centers', 'to', 'find', 'out', 'what', 'local', 'youth', 'services', 'were', 'around.', '', '', '', \"Didn't\", 'have', 'to', 'find', 'anything', 'new', 'mostly', 'because', 'they', \"weren't\", 'involved', 'in', 'anything', 'too', 'niche.', '', 'It', 'happened', 'pretty', 'quickly,', 'but', 'it', 'always', 'took', 'a', 'long', 'time', 'to', 'find', 'the', 'right', 'or', 'best', 'fit', 'for', 'the', 'kids', 'based', 'on', 'the', 'acticity', 'or', 'resources.', '', 'Finding', 'basketball', 'was', 'easy...finding', 'the', 'right', 'basketball', 'team,', 'location,', 'coach,', 'league,', 'etc....always', 'took', 'a', 'long', 'time', 'to', 'do', 'right', '', '', 'It', 'was', 'always', 'anxious', 'to', 'trade', 'in', 'a', 'known', 'for', 'an', 'unknown.', '', 'Always', 'worried', 'about', 'the', 'kids', 'getting', 'a', 'fair', 'shake', 'or', 'opportunity', 'to', 'be', 'included', 'in', 'band,', 'sports,', 'clubs,', 'etc....very', 'much', 'had', 'to', 'rely', 'on', 'fate', 'and', 'that', 'it', 'would', 'all', 'work', 'out', 'with', 'a', 'little', 'luck.', '', 'Where', 'to', 'live', 'and', 'finding', 'a', 'school', 'district', 'were', 'always', 'incredibly', 'frustrating', 'and', 'stressful,', 'so', 'was', 'finding', 'a', 'doctor.', '', '', '', 'Deployments', 'were', 'the', 'hardest.', '', 'Finding,', 'then', 'selling/buying', 'a', 'house', 'in', 'a', \"'good'\", 'school', 'district', 'was', 'usually', 'the', 'next', 'hardest.', '', 'And,', 'the', 'anxiousness', 'of', 'wondering', 'if', 'the', 'move', 'was', 'the', 'right', 'thing', 'for', 'the', 'kids....did', 'we', 'move', 'them', 'early?', '', 'did', 'we', 'give', 'them', 'the', 'right', 'opportunities', 'at', 'the', 'right', 'time?', 'did', 'we', 'take', 'something', 'from', 'them', 'early?', '', '', '', '', '-', 'base', 'or', 'school', 'really', 'depends', 'on', 'the', 'geography,', 'age', 'and', 'message', \"you're\", 'trying', 'to', 'hit....the', 'bases', 'should', 'have', 'sponsor', 'programs', '(receiving', 'sponsor', 'for', 'an', 'inbound', 'service', 'member/family)', 'but', 'they', 'are', 'very', 'personality', 'and', 'command', 'dependent.', '', 'Very', 'easy', 'to', 'fail,', 'very', 'time', 'consuming', 'and', 'invested', 'to', 'succeed.', 'Schools', 'can', 'be', 'a', 'great', 'inject', 'point,', 'some', 'have', 'Military', 'Family', 'Liaison', 'Counselors', 'or', 'established', 'programs....but', 'if', 'the', 'target', 'is', 'the', 'parents...they', 'often', 'times', \"don't\", 'rummage', 'through', 'back', 'packs', 'or', 'read', 'the', 'school', 'flyers/hand', 'outs.', '', '\\n-', 'knowing', 'someone', 'absolutely', 'expedited', 'settling', 'in', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['famili', 'move', 'omaha', 'mead', 'mead', 'bragg', 'bragg', 'joint', 'base', 'antonio', 'randolph', 'famili', 'kid', 'month', 'respect', 'wife', 'reli', 'sponsor', 'packag', 'materi', 'internet', 'famili', 'support', 'center', 'purpos', 'stay', 'base', 'idea', 'expect', 'subsequ', 'move', 'internet', 'help', 'reli', 'heavili', 'friend', 'network', 'grind', 'truth', 'school', 'district', 'doctor', 'neighborhood', 'commut', 'time', 'reli', 'branch', 'specif', 'difficult', 'militari', 'spous', 'close', 'relationship', 'cast', 'societ', 'spous', 'lot', 'bona', 'fide', 'exchang', 'fear', 'move', 'general', 'guard', 'close', 'outsid', 'immedi', 'peer', 'neighbor', 'kid', 'pretti', 'adapt', 'normal', 'hous', 'offic', 'save', 'util', 'start', 'cost', 'usual', 'child', 'develop', 'center', 'local', 'youth', 'servic', 'weren', 'involv', 'nich', 'happen', 'pretti', 'quick', 'take', 'long', 'time', 'right', 'best', 'kid', 'base', 'actic', 'resourc', 'find', 'basketbal', 'easi', 'find', 'right', 'basketbal', 'team', 'locat', 'coach', 'leagu', 'take', 'long', 'time', 'right', 'anxious', 'trade', 'know', 'unknown', 'worri', 'kid', 'get', 'fair', 'shake', 'opportun', 'includ', 'band', 'sport', 'club', 'reli', 'fate', 'work', 'littl', 'luck', 'live', 'find', 'school', 'district', 'incred', 'frustrat', 'stress', 'find', 'doctor', 'deploy', 'hardest', 'find', 'sell', 'buy', 'hous', 'good', 'school', 'district', 'usual', 'hardest', 'anxious', 'wonder', 'right', 'thing', 'kid', 'earli', 'right', 'opportun', 'right', 'time', 'earli', 'base', 'school', 'depend', 'geographi', 'messag', 'tri', 'base', 'sponsor', 'program', 'receiv', 'sponsor', 'inbound', 'servic', 'member', 'famili', 'person', 'command', 'depend', 'easi', 'fail', 'time', 'consum', 'invest', 'succeed', 'school', 'great', 'inject', 'point', 'militari', 'famili', 'liaison', 'counselor', 'establish', 'program', 'target', 'parent', 'time', 'rummag', 'pack', 'read', 'school', 'flyer', 'hand', 'out', 'know', 'absolut', 'expedit', 'settl']\n"
     ]
    }
   ],
   "source": [
    "#testing preprocessing on a doc \n",
    "doc_sample = allText[0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words: create a dictionary for the number of times a word appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 academ\n",
      "1 actual\n",
      "2 anxieti\n",
      "3 area\n",
      "4 autist\n",
      "5 awar\n",
      "6 brother\n",
      "7 build\n",
      "8 care\n",
      "9 child\n",
      "10 china\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count += 1\n",
    "    if count > 10: \n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim filter_extremes\n",
    "#filter parameters:\n",
    "no_below = 5  #absolute number\n",
    "no_above = 0.4 # fraction of total corpus size \n",
    "dictionary.filter_extremes(no_below, no_above, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1),\n",
       " (37, 1),\n",
       " (40, 1),\n",
       " (43, 1),\n",
       " (51, 1),\n",
       " (52, 3),\n",
       " (58, 1),\n",
       " (65, 1),\n",
       " (66, 2),\n",
       " (68, 1),\n",
       " (73, 1),\n",
       " (76, 1),\n",
       " (79, 2),\n",
       " (80, 1),\n",
       " (83, 1),\n",
       " (85, 1),\n",
       " (86, 2),\n",
       " (93, 2),\n",
       " (104, 2),\n",
       " (107, 2),\n",
       " (110, 1),\n",
       " (127, 2),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (146, 1),\n",
       " (150, 1),\n",
       " (156, 1),\n",
       " (158, 1),\n",
       " (167, 1),\n",
       " (182, 2),\n",
       " (187, 1),\n",
       " (195, 1),\n",
       " (197, 1),\n",
       " (205, 3),\n",
       " (234, 1),\n",
       " (235, 1),\n",
       " (236, 1),\n",
       " (255, 1),\n",
       " (264, 1),\n",
       " (267, 1),\n",
       " (276, 1),\n",
       " (283, 3),\n",
       " (284, 2),\n",
       " (287, 3),\n",
       " (294, 3),\n",
       " (307, 1),\n",
       " (311, 1),\n",
       " (316, 2),\n",
       " (323, 1),\n",
       " (333, 1),\n",
       " (350, 1),\n",
       " (365, 1),\n",
       " (386, 1)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 4 (\"build\") appears 1 time.\n",
      "Word 37 (\"children\") appears 1 time.\n",
      "Word 40 (\"close\") appears 1 time.\n",
      "Word 43 (\"depend\") appears 1 time.\n",
      "Word 51 (\"idea\") appears 1 time.\n",
      "Word 52 (\"incom\") appears 3 time.\n",
      "Word 58 (\"offer\") appears 1 time.\n",
      "Word 65 (\"provid\") appears 1 time.\n",
      "Word 66 (\"question\") appears 2 time.\n",
      "Word 68 (\"relat\") appears 1 time.\n",
      "Word 73 (\"travel\") appears 1 time.\n",
      "Word 76 (\"wife\") appears 1 time.\n",
      "Word 79 (\"call\") appears 2 time.\n",
      "Word 80 (\"counselor\") appears 1 time.\n",
      "Word 83 (\"find\") appears 1 time.\n",
      "Word 85 (\"guidanc\") appears 1 time.\n",
      "Word 86 (\"issu\") appears 2 time.\n",
      "Word 93 (\"open\") appears 2 time.\n",
      "Word 104 (\"allen\") appears 2 time.\n",
      "Word 107 (\"booz\") appears 2 time.\n",
      "Word 110 (\"choos\") appears 1 time.\n",
      "Word 127 (\"right\") appears 2 time.\n",
      "Word 139 (\"convers\") appears 1 time.\n",
      "Word 140 (\"day\") appears 1 time.\n",
      "Word 146 (\"involv\") appears 1 time.\n",
      "Word 150 (\"month\") appears 1 time.\n",
      "Word 156 (\"similar\") appears 1 time.\n",
      "Word 158 (\"tell\") appears 1 time.\n",
      "Word 167 (\"address\") appears 1 time.\n",
      "Word 182 (\"district\") appears 2 time.\n",
      "Word 187 (\"figur\") appears 1 time.\n",
      "Word 195 (\"happen\") appears 1 time.\n",
      "Word 197 (\"inform\") appears 1 time.\n",
      "Word 205 (\"look\") appears 3 time.\n",
      "Word 234 (\"wasn\") appears 1 time.\n",
      "Word 235 (\"younger\") appears 1 time.\n",
      "Word 236 (\"arriv\") appears 1 time.\n",
      "Word 255 (\"project\") appears 1 time.\n",
      "Word 264 (\"typic\") appears 1 time.\n",
      "Word 267 (\"acknowledg\") appears 1 time.\n",
      "Word 276 (\"interact\") appears 1 time.\n",
      "Word 283 (\"schedul\") appears 3 time.\n",
      "Word 284 (\"tour\") appears 2 time.\n",
      "Word 287 (\"ambassador\") appears 3 time.\n",
      "Word 294 (\"advanc\") appears 3 time.\n",
      "Word 307 (\"link\") appears 1 time.\n",
      "Word 311 (\"play\") appears 1 time.\n",
      "Word 316 (\"regist\") appears 2 time.\n",
      "Word 323 (\"miss\") appears 1 time.\n",
      "Word 333 (\"one\") appears 1 time.\n",
      "Word 350 (\"opportun\") appears 1 time.\n",
      "Word 365 (\"ask\") appears 1 time.\n",
      "Word 386 (\"somewhat\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_23 = bow_corpus[23]\n",
    "\n",
    "for i in range(len(bow_doc_23)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_23[i][0], \n",
    "                                                     dictionary[bow_doc_23[i][0]], \n",
    "                                                     bow_doc_23[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.19404372710131615),\n",
      " (1, 0.19404372710131615),\n",
      " (2, 0.156207625816503),\n",
      " (3, 0.17322101797112616),\n",
      " (4, 0.19929175371604044),\n",
      " (5, 0.22088886818360837),\n",
      " (6, 0.09152638344939759),\n",
      " (7, 0.14182300771143277),\n",
      " (8, 0.22088886818360837),\n",
      " (9, 0.10853977560402078),\n",
      " (10, 0.14182300771143277),\n",
      " (11, 0.17322101797112616),\n",
      " (12, 0.14182300771143277),\n",
      " (13, 0.14182300771143277),\n",
      " (14, 0.19404372710131615),\n",
      " (15, 0.18305276689879518),\n",
      " (16, 0.312415251633006),\n",
      " (17, 0.09152638344939759),\n",
      " (18, 0.11837152453168984),\n",
      " (19, 0.09964587685802022),\n",
      " (20, 0.19404372710131615),\n",
      " (21, 0.12936248473421075),\n",
      " (22, 0.21707955120804157),\n",
      " (23, 0.19404372710131615),\n",
      " (24, 0.10853977560402078),\n",
      " (25, 0.10853977560402078),\n",
      " (26, 0.156207625816503),\n",
      " (27, 0.14182300771143277),\n",
      " (28, 0.17322101797112616),\n",
      " (29, 0.11837152453168984),\n",
      " (30, 0.17322101797112616),\n",
      " (31, 0.3464420359422523)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models \n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint \n",
    "\n",
    "for doc in corpus_tfidf: \n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running LDA with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = n, \n",
    "                                       id2word=dictionary, passes=3, workers=2,\n",
    "                                      per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.012*\"feel\" + 0.012*\"look\" + 0.011*\"understand\" + 0.011*\"home\" + 0.011*\"summer\" + 0.011*\"brother\" + 0.010*\"civilian\" + 0.009*\"daughter\" + 0.009*\"post\" + 0.009*\"life\"\n",
      "Topic: 1 \n",
      "Words: 0.021*\"armi\" + 0.019*\"train\" + 0.016*\"provid\" + 0.014*\"liaison\" + 0.013*\"transfer\" + 0.013*\"command\" + 0.013*\"state\" + 0.012*\"youth\" + 0.011*\"issu\" + 0.011*\"counti\"\n",
      "Topic: 2 \n",
      "Words: 0.021*\"right\" + 0.021*\"organ\" + 0.018*\"instal\" + 0.018*\"typic\" + 0.018*\"level\" + 0.016*\"guidanc\" + 0.015*\"command\" + 0.015*\"type\" + 0.015*\"look\" + 0.013*\"district\"\n",
      "Topic: 3 \n",
      "Words: 0.019*\"sponsor\" + 0.014*\"youth\" + 0.014*\"specif\" + 0.014*\"life\" + 0.012*\"packet\" + 0.012*\"anchor\" + 0.012*\"send\" + 0.012*\"reach\" + 0.011*\"train\" + 0.011*\"involv\"\n",
      "Topic: 4 \n",
      "Words: 0.016*\"navi\" + 0.014*\"older\" + 0.013*\"relationship\" + 0.012*\"network\" + 0.012*\"hard\" + 0.012*\"club\" + 0.012*\"involv\" + 0.011*\"diego\" + 0.011*\"middl\" + 0.010*\"build\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\605665\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1      0.004506 -0.066477       1        1  26.840738\n",
       "0     -0.057225  0.046662       2        1  25.856102\n",
       "4     -0.068648  0.066108       3        1  19.627153\n",
       "3     -0.013597 -0.092594       4        1  17.982069\n",
       "2      0.134963  0.046301       5        1   9.693944, topic_info=    Category       Freq        Term      Total  loglift  logprob\n",
       "254  Default  26.000000       organ  26.000000  30.0000  30.0000\n",
       "127  Default  22.000000       right  22.000000  29.0000  29.0000\n",
       "264  Default  15.000000       typic  15.000000  28.0000  28.0000\n",
       "250  Default  20.000000      instal  20.000000  27.0000  27.0000\n",
       "203  Default  30.000000       level  30.000000  26.0000  26.0000\n",
       "263  Default  17.000000        type  17.000000  25.0000  25.0000\n",
       "260  Default  24.000000        slos  24.000000  24.0000  24.0000\n",
       "361  Default  38.000000       youth  38.000000  23.0000  23.0000\n",
       "170  Default  46.000000        armi  46.000000  22.0000  22.0000\n",
       "41   Default  43.000000     command  43.000000  21.0000  21.0000\n",
       "60   Default  16.000000      packet  16.000000  20.0000  20.0000\n",
       "85   Default  26.000000     guidanc  26.000000  19.0000  19.0000\n",
       "205  Default  46.000000        look  46.000000  18.0000  18.0000\n",
       "382  Default  15.000000      anchor  15.000000  17.0000  17.0000\n",
       "128  Default  40.000000     sponsor  40.000000  16.0000  16.0000\n",
       "132  Default  50.000000       train  50.000000  15.0000  15.0000\n",
       "51   Default  21.000000        idea  21.000000  14.0000  14.0000\n",
       "321  Default  33.000000        life  33.000000  13.0000  13.0000\n",
       "7    Default  19.000000        club  19.000000  12.0000  12.0000\n",
       "208  Default  34.000000     network  34.000000  11.0000  11.0000\n",
       "88   Default  32.000000     liaison  32.000000  10.0000  10.0000\n",
       "69   Default  22.000000        send  22.000000   9.0000   9.0000\n",
       "123  Default  23.000000        make  23.000000   8.0000   8.0000\n",
       "215  Default  19.000000        peer  19.000000   7.0000   7.0000\n",
       "67   Default  36.000000       reach  36.000000   6.0000   6.0000\n",
       "52   Default  13.000000       incom  13.000000   5.0000   5.0000\n",
       "287  Default  26.000000  ambassador  26.000000   4.0000   4.0000\n",
       "178  Default  22.000000      counti  22.000000   3.0000   3.0000\n",
       "195  Default   9.000000      happen   9.000000   2.0000   2.0000\n",
       "22   Default  34.000000       older  34.000000   1.0000   1.0000\n",
       "..       ...        ...         ...        ...      ...      ...\n",
       "261   Topic5   3.612277        star  11.789295   1.1508  -5.0593\n",
       "222   Topic5   3.570770         see  11.857639   1.1335  -5.0708\n",
       "93    Topic5   3.567901        open  11.920814   1.1274  -5.0716\n",
       "255   Topic5   3.504355     project  11.739172   1.1247  -5.0896\n",
       "95    Topic5   1.870319    previous   6.333838   1.1139  -5.7175\n",
       "256   Topic5   1.864051     quarter   6.336238   1.1101  -5.7208\n",
       "140   Topic5   1.862336         day   6.386078   1.1014  -5.7218\n",
       "121   Topic5   3.545064      letter  12.160346   1.1010  -5.0780\n",
       "362   Topic5   1.847435    carolina   6.340361   1.1005  -5.7298\n",
       "152   Topic5   1.853873      pcsing   6.380464   1.0977  -5.7263\n",
       "276   Topic5   5.107769    interact  17.623770   1.0952  -4.7128\n",
       "260   Topic5   6.886969        slos  24.826406   1.0514  -4.4140\n",
       "49    Topic5   5.193126       great  19.587013   1.0061  -4.6963\n",
       "248   Topic5   3.582512   implement  13.542730   1.0039  -5.0675\n",
       "236   Topic5   3.581259       arriv  13.837378   0.9820  -5.0679\n",
       "122   Topic5   6.771924       locat  30.709724   0.8219  -4.4308\n",
       "41    Topic5   8.631090     command  43.297997   0.7209  -4.1882\n",
       "205   Topic5   8.527459        look  46.525997   0.6369  -4.2003\n",
       "67    Topic5   6.964468       reach  36.351246   0.6813  -4.4028\n",
       "182   Topic5   7.157665    district  47.468498   0.4418  -4.3754\n",
       "287   Topic5   5.253339  ambassador  26.551722   0.7134  -4.6847\n",
       "80    Topic5   5.428535   counselor  32.880001   0.5325  -4.6519\n",
       "208   Topic5   5.494239     network  34.759724   0.4889  -4.6399\n",
       "86    Topic5   5.292464        issu  35.872375   0.4200  -4.6773\n",
       "66    Topic5   3.740612    question  19.766632   0.6689  -5.0244\n",
       "257   Topic5   3.623290       retir  14.047780   0.9786  -5.0562\n",
       "79    Topic5   3.729357        call  21.572372   0.5785  -5.0274\n",
       "132   Topic5   3.946038       train  50.498817  -0.2156  -4.9709\n",
       "42    Topic5   3.860219    daughter  44.129547  -0.1027  -4.9929\n",
       "65    Topic5   3.756958      provid  41.187302  -0.0609  -5.0200\n",
       "\n",
       "[285 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "32        1  0.167085         abl\n",
       "32        2  0.501255         abl\n",
       "32        3  0.125314         abl\n",
       "32        4  0.125314         abl\n",
       "32        5  0.083543         abl\n",
       "0         2  0.141527      actual\n",
       "0         3  0.283053      actual\n",
       "0         4  0.495343      actual\n",
       "293       1  0.117030       adapt\n",
       "293       2  0.234059       adapt\n",
       "293       3  0.117030       adapt\n",
       "293       4  0.468118       adapt\n",
       "294       1  0.143577      advanc\n",
       "294       2  0.287153      advanc\n",
       "294       3  0.071788      advanc\n",
       "294       4  0.143577      advanc\n",
       "294       5  0.358942      advanc\n",
       "104       1  0.118278       allen\n",
       "104       2  0.473112       allen\n",
       "104       5  0.354834       allen\n",
       "136       1  0.237517       allow\n",
       "136       2  0.118758       allow\n",
       "136       4  0.593791       allow\n",
       "287       1  0.414286  ambassador\n",
       "287       2  0.037662  ambassador\n",
       "287       3  0.075325  ambassador\n",
       "287       4  0.301299  ambassador\n",
       "287       5  0.188312  ambassador\n",
       "382       1  0.125189      anchor\n",
       "382       2  0.062594      anchor\n",
       "...     ...       ...         ...\n",
       "162       3  0.173801        unit\n",
       "162       4  0.086900        unit\n",
       "162       5  0.057934        unit\n",
       "163       1  0.425288       usual\n",
       "163       2  0.255173       usual\n",
       "163       3  0.042529       usual\n",
       "163       4  0.212644       usual\n",
       "164       1  0.515877    virginia\n",
       "164       2  0.103175    virginia\n",
       "164       3  0.309526    virginia\n",
       "164       4  0.051588    virginia\n",
       "102       1  0.479747      websit\n",
       "102       4  0.479747      websit\n",
       "165       1  0.144103        week\n",
       "165       2  0.072051        week\n",
       "165       3  0.720514        week\n",
       "75        1  0.108614      welcom\n",
       "75        2  0.434454      welcom\n",
       "75        3  0.181023      welcom\n",
       "75        4  0.144818      welcom\n",
       "75        5  0.144818      welcom\n",
       "376       2  0.350533       weren\n",
       "376       3  0.467377       weren\n",
       "326       1  0.238593        wish\n",
       "326       2  0.238593        wish\n",
       "326       3  0.477186        wish\n",
       "361       1  0.499979       youth\n",
       "361       2  0.052629       youth\n",
       "361       3  0.026315       youth\n",
       "361       4  0.394720       youth\n",
       "\n",
       "[759 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1, 5, 4, 3])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
